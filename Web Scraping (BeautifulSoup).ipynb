{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping for Indeed.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hello~ I'm Tako. I am an aspiring data analyst with a strong background in qualitative communication research. The transition to data science from a purely qualitative field has been a wild ride but I sincerely love the challenges. Here is my first project and I'm very proud because I finished it with very minimum help of the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This project showcases two major skills. 1. Collect unstructured data by web scraping a website 2. Visualization of the result \\\n",
    "Since I'm going into data science, I want to know what is most the fundamental skills I need to acquire before applying for a job position. I chose Indeed.com and scraped the top 200 Data Scientist job required skills in 7 cities (Los Angeles, Boston, Austin, Chicago, San Francisco, DC and New York). Then I used matplotlib to graph the frequency of each required programming languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Set up a request to the URL Indeed.com/m/. Then use BeautifulSoup to parse the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the packages needed\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import urlopen\n",
    "import lxml\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_result_per_city = 20\n",
    "cities = []\n",
    "title = []\n",
    "company = []\n",
    "location = []\n",
    "jd = []\n",
    "for city in set(['Los+Angeles', 'New+York', 'San+Francisco', 'Chicago', 'Boston', 'Austin', 'DC']):\n",
    "    \n",
    "    url = \"https://www.indeed.com/m/jobs?q=data+scientist&l=\" + str(city)\n",
    "    print(city)\n",
    "    \n",
    "    for i in range(0, max_result_per_city):\n",
    "        page = urlopen(url)\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "        all_matches = soup.findAll(attrs={'rel':['nofollow']})\n",
    "        for each in all_matches:\n",
    "            jd_url= 'http://www.indeed.com/m/'+ each['href']\n",
    "            jd_page =urlopen(jd_url)\n",
    "            jd_soup = BeautifulSoup(jd_page, 'lxml')\n",
    "            jd_desc = jd_soup.findAll(attrs={'id':['desc']})\n",
    "            cities.append(city)\n",
    "            title.append(jd_soup.body.p.b.font.text)\n",
    "            company.append(jd_desc[0].span.text)\n",
    "            location.append(jd_soup.body.p.span.text)\n",
    "            jd.append(jd_desc[0].text)\n",
    "        \n",
    "        url = 'http://www.indeed.com/m/jobs?q=data+scientist&l='+ str(city) + '&start=' + str((i+1)*10)\n",
    "        print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create a dataframe to store all the information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a51ef6a2e80f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m job = {'cities': cities,'title': title,\n\u001b[0m\u001b[1;32m      2\u001b[0m          \u001b[0;34m'company'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcompany\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m          \u001b[0;34m'location'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m          'Job_Description': jd}\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cities' is not defined"
     ]
    }
   ],
   "source": [
    "job = {'cities': cities,'title': title,\n",
    "         'company': company,\n",
    "         'location': location,\n",
    "         'Job_Description': jd}\n",
    "df = pd.DataFrame.from_dict(job)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Since I only want 200 entries, I slice the dataframe according to the city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SF = df[df.cities == 'San+Francisco']\n",
    "LA = df[df.cities == 'Los+Angeles']\n",
    "BOS = df[df.cities == 'Boston']\n",
    "DC = df[df.cities == 'DC']\n",
    "NY = df[df.cities == 'New+York']\n",
    "AUS = df[df.cities == 'Austin']\n",
    "CHI = df[df.cities == 'Chicago']\n",
    "print(AUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Find the Key word (C, C++, Java, Javascript, Python, R, SQL, Hadoop, Hive, Pig, Spark, AWS and Tableau)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Los Angeles Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find out the frequency of each skills for LA\n",
    "\n",
    "freq_Python = LA.Job_Description.str.contains(r'Python|python|PYTHON').sum()\n",
    "freq_Java = LA.Job_Description.str.contains(r'Java|JAVA|java').sum()\n",
    "freq_hadoop = LA.Job_Description.str.contains(r'[Hh]adoop').sum()\n",
    "freq_cplus = LA.Job_Description.str.contains(r'C\\+\\+').sum()\n",
    "freq_hive = LA.Job_Description.str.contains(r'[Hh]ive').sum()\n",
    "freq_spark = LA.Job_Description.str.contains(r'[Ss]park').sum()\n",
    "freq_aws = LA.Job_Description.str.contains(r'AWS').sum()\n",
    "freq_tableau = LA.Job_Description.str.contains(r'[Tt]ableau').sum()\n",
    "freq_sql = LA.Job_Description.str.contains(r'SQL|sql').sum()\n",
    "freq_pig = LA.Job_Description.str.contains(r'[Pp]ig').sum()\n",
    "freq_c = LA.Job_Description.str.contains(r'\\bC(?!.)\\b').sum()\n",
    "freq_r = LA.Job_Description.str.contains(r'\\s[Rr]\\s|\\s[Rr]\\.').sum()\n",
    "freq_javascript = LA.Job_Description.str.contains(r'[Jj]avascript').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Table\n",
    "from pandas import DataFrame\n",
    "\n",
    "freq_table = {'Skill': ['Python', 'Java', 'Hadoop', 'C++', 'Hive', 'Spark', 'AWS', 'Tableau', 'SQL', 'Pig', 'C', 'R', 'Javascript'], \n",
    "              'LA_Frequency' : [freq_Python, freq_Java, freq_hadoop, freq_cplus, freq_hive, freq_spark, freq_aws,freq_tableau, freq_sql,freq_pig, freq_c, freq_r, freq_javascript]}\n",
    "LA_table = DataFrame(freq_table, columns = ['Skill', 'LA_Frequency'])\n",
    "print(LA_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boston Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the frequency\n",
    "freq_Python = BOS.Job_Description.str.contains(r'Python|python|PYTHON').sum()\n",
    "freq_Java = BOS.Job_Description.str.contains(r'Java|JAVA|java').sum()\n",
    "freq_hadoop = BOS.Job_Description.str.contains(r'[Hh]adoop').sum()\n",
    "freq_cplus = BOS.Job_Description.str.contains(r'C\\+\\+').sum()\n",
    "freq_hive = BOS.Job_Description.str.contains(r'[Hh]ive').sum()\n",
    "freq_spark = BOS.Job_Description.str.contains(r'[Ss]park').sum()\n",
    "freq_aws = BOS.Job_Description.str.contains(r'AWS').sum()\n",
    "freq_tableau = BOS.Job_Description.str.contains(r'[Tt]ableau').sum()\n",
    "freq_sql = BOS.Job_Description.str.contains(r'SQL|sql').sum()\n",
    "freq_pig = BOS.Job_Description.str.contains(r'[Pp]ig').sum()\n",
    "freq_c = BOS.Job_Description.str.contains(r'\\bC(?!.)\\b').sum()\n",
    "freq_r = BOS.Job_Description.str.contains(r'\\s[Rr]\\s|\\s[Rr]\\.').sum()\n",
    "freq_javascript = BOS.Job_Description.str.contains(r'[Jj]avascript').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Table\n",
    "from pandas import DataFrame\n",
    "\n",
    "freq_table = {'Skill': ['Python', 'Java', 'Hadoop', 'C++', 'Hive', 'Spark', 'AWS', 'Tableau', 'SQL', 'Pig', 'C', 'R', 'Javascript'], \n",
    "              'BOS_Frequency' : [freq_Python, freq_Java, freq_hadoop, freq_cplus, freq_hive, freq_spark, freq_aws,freq_tableau, freq_sql,freq_pig, freq_c, freq_r, freq_javascript]}\n",
    "BOS_table = DataFrame(freq_table, columns = ['Skill', 'BOS_Frequency'])\n",
    "print(BOS_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### San Francisco Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the frequency\n",
    "freq_Python = SF.Job_Description.str.contains(r'Python|python|PYTHON').sum()\n",
    "freq_Java = SF.Job_Description.str.contains(r'Java|JAVA|java').sum()\n",
    "freq_hadoop = SF.Job_Description.str.contains(r'[Hh]adoop').sum()\n",
    "freq_cplus = SF.Job_Description.str.contains(r'C\\+\\+').sum()\n",
    "freq_hive = SF.Job_Description.str.contains(r'[Hh]ive').sum()\n",
    "freq_spark = SF.Job_Description.str.contains(r'[Ss]park').sum()\n",
    "freq_aws = SF.Job_Description.str.contains(r'AWS').sum()\n",
    "freq_tableau = SF.Job_Description.str.contains(r'[Tt]ableau').sum()\n",
    "freq_sql = SF.Job_Description.str.contains(r'SQL|sql').sum()\n",
    "freq_pig = SF.Job_Description.str.contains(r'[Pp]ig').sum()\n",
    "freq_c = SF.Job_Description.str.contains(r'\\bC(?!.)\\b').sum()\n",
    "freq_r = SF.Job_Description.str.contains(r'\\s[Rr]\\s|\\s[Rr]\\.').sum()\n",
    "freq_javascript = SF.Job_Description.str.contains(r'[Jj]avascript').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Table\n",
    "from pandas import DataFrame\n",
    "\n",
    "freq_table = {'Skill': ['Python', 'Java', 'Hadoop', 'C++', 'Hive', 'Spark', 'AWS', 'Tableau', 'SQL', 'Pig', 'C', 'R', 'Javascript'], \n",
    "              'SF_Frequency' : [freq_Python, freq_Java, freq_hadoop, freq_cplus, freq_hive, freq_spark, freq_aws,freq_tableau, freq_sql,freq_pig, freq_c, freq_r, freq_javascript]}\n",
    "SF_table = DataFrame(freq_table, columns = ['Skill', 'SF_Frequency'])\n",
    "print(SF_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chicago Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the frequency\n",
    "freq_Python = CHI.Job_Description.str.contains(r'Python|python|PYTHON').sum()\n",
    "freq_Java = CHI.Job_Description.str.contains(r'Java|JAVA|java').sum()\n",
    "freq_hadoop = CHI.Job_Description.str.contains(r'[Hh]adoop').sum()\n",
    "freq_cplus = CHI.Job_Description.str.contains(r'C\\+\\+').sum()\n",
    "freq_hive = CHI.Job_Description.str.contains(r'[Hh]ive').sum()\n",
    "freq_spark = CHI.Job_Description.str.contains(r'[Ss]park').sum()\n",
    "freq_aws = CHI.Job_Description.str.contains(r'AWS').sum()\n",
    "freq_tableau = CHI.Job_Description.str.contains(r'[Tt]ableau').sum()\n",
    "freq_sql = CHI.Job_Description.str.contains(r'SQL|sql').sum()\n",
    "freq_pig = CHI.Job_Description.str.contains(r'[Pp]ig').sum()\n",
    "freq_c = CHI.Job_Description.str.contains(r'\\bC(?!.)\\b').sum()\n",
    "freq_r = CHI.Job_Description.str.contains(r'\\s[Rr]\\s|\\s[Rr]\\.').sum()\n",
    "freq_javascript = CHI.Job_Description.str.contains(r'[Jj]avascript').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Table\n",
    "from pandas import DataFrame\n",
    "\n",
    "freq_table = {'Skill': ['Python', 'Java', 'Hadoop', 'C++', 'Hive', 'Spark', 'AWS', 'Tableau', 'SQL', 'Pig', 'C', 'R', 'Javascript'], \n",
    "              'CHI_Frequency' : [freq_Python, freq_Java, freq_hadoop, freq_cplus, freq_hive, freq_spark, freq_aws,freq_tableau, freq_sql,freq_pig, freq_c, freq_r, freq_javascript]}\n",
    "CHI_table = DataFrame(freq_table, columns = ['Skill', 'CHI_Frequency'])\n",
    "print(CHI_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New York Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find out the frequency of each skills on JD\n",
    "freq_Python = NY.Job_Description.str.contains(r'Python|python|PYTHON').sum()\n",
    "freq_Java = NY.Job_Description.str.contains(r'Java|JAVA|java').sum()\n",
    "freq_hadoop = NY.Job_Description.str.contains(r'[Hh]adoop').sum()\n",
    "freq_cplus = NY.Job_Description.str.contains(r'C\\+\\+').sum()\n",
    "freq_hive = NY.Job_Description.str.contains(r'[Hh]ive').sum()\n",
    "freq_spark = NY.Job_Description.str.contains(r'[Ss]park').sum()\n",
    "freq_aws = NY.Job_Description.str.contains(r'AWS').sum()\n",
    "freq_tableau = NY.Job_Description.str.contains(r'[Tt]ableau').sum()\n",
    "freq_sql = df.Job_Description.str.contains(r'SQL|sql').sum()\n",
    "freq_pig = df.Job_Description.str.contains(r'[Pp]ig').sum()\n",
    "freq_c = df.Job_Description.str.contains(r'\\bC(?!.)\\b').sum()\n",
    "freq_r = df.Job_Description.str.contains(r'\\s[Rr]\\s|\\s[Rr]\\.').sum()\n",
    "freq_javascript = df.Job_Description.str.contains(r'[Jj]avascript').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Table\n",
    "from pandas import DataFrame\n",
    "\n",
    "freq_table = {'Skill': ['Python', 'Java', 'Hadoop', 'C++', 'Hive', 'Spark', 'AWS', 'Tableau', 'SQL', 'Pig', 'C', 'R', 'Javascript'], \n",
    "              'NY_Frequency' : [freq_Python, freq_Java, freq_hadoop, freq_cplus, freq_hive, freq_spark, freq_aws,freq_tableau, freq_sql,freq_pig, freq_c, freq_r, freq_javascript]}\n",
    "NY_table = DataFrame(freq_table, columns = ['Skill', 'NY_Frequency'])\n",
    "print(NY_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DC Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find out the frequency of each skills on JD\n",
    "freq_Python = DC.Job_Description.str.contains(r'Python|python|PYTHON').sum()\n",
    "freq_Java = DC.Job_Description.str.contains(r'Java|JAVA|java').sum()\n",
    "freq_hadoop = DC.Job_Description.str.contains(r'[Hh]adoop').sum()\n",
    "freq_cplus = DC.Job_Description.str.contains(r'C\\+\\+').sum()\n",
    "freq_hive = DC.Job_Description.str.contains(r'[Hh]ive').sum()\n",
    "freq_spark = DC.Job_Description.str.contains(r'[Ss]park').sum()\n",
    "freq_aws = DC.Job_Description.str.contains(r'AWS').sum()\n",
    "freq_tableau = DC.Job_Description.str.contains(r'[Tt]ableau').sum()\n",
    "freq_sql = DC.Job_Description.str.contains(r'SQL|sql').sum()\n",
    "freq_pig = DC.Job_Description.str.contains(r'[Pp]ig').sum()\n",
    "freq_c = DC.Job_Description.str.contains(r'\\bC(?!.)\\b').sum()\n",
    "freq_r = DC.Job_Description.str.contains(r'\\s[Rr]\\s|\\s[Rr]\\.').sum()\n",
    "freq_javascript = DC.Job_Description.str.contains(r'[Jj]avascript').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Table\n",
    "from pandas import DataFrame\n",
    "\n",
    "freq_table = {'Skill': ['Python', 'Java', 'Hadoop', 'C++', 'Hive', 'Spark', 'AWS', 'Tableau', 'SQL', 'Pig', 'C', 'R', 'Javascript'], \n",
    "              'DC_Frequency' : [freq_Python, freq_Java, freq_hadoop, freq_cplus, freq_hive, freq_spark, freq_aws,freq_tableau, freq_sql,freq_pig, freq_c, freq_r, freq_javascript]}\n",
    "DC_table = DataFrame(freq_table, columns = ['Skill', 'DC_Frequency'])\n",
    "print(DC_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Austin Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find out the frequency of each skills on JD\n",
    "freq_Python = AUS.Job_Description.str.contains(r'Python|python|PYTHON').sum()\n",
    "freq_Java = AUS.Job_Description.str.contains(r'Java|JAVA|java').sum()\n",
    "freq_hadoop = AUS.Job_Description.str.contains(r'[Hh]adoop').sum()\n",
    "freq_cplus = AUS.Job_Description.str.contains(r'C\\+\\+').sum()\n",
    "freq_hive = AUS.Job_Description.str.contains(r'[Hh]ive').sum()\n",
    "freq_spark = AUS.Job_Description.str.contains(r'[Ss]park').sum()\n",
    "freq_aws = AUS.Job_Description.str.contains(r'AWS').sum()\n",
    "freq_tableau = AUS.Job_Description.str.contains(r'[Tt]ableau').sum()\n",
    "freq_sql = AUS.Job_Description.str.contains(r'SQL|sql').sum()\n",
    "freq_pig = AUS.Job_Description.str.contains(r'[Pp]ig').sum()\n",
    "freq_c = AUS.Job_Description.str.contains(r'\\bC(?!.)\\b').sum()\n",
    "freq_r = AUS.Job_Description.str.contains(r'\\s[Rr]\\s|\\s[Rr]\\.').sum()\n",
    "freq_javascript = AUS.Job_Description.str.contains(r'[Jj]avascript').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Table\n",
    "from pandas import DataFrame\n",
    "\n",
    "freq_table = {'Skill': ['Python', 'Java', 'Hadoop', 'C++', 'Hive', 'Spark', 'AWS', 'Tableau', 'SQL', 'Pig', 'C', 'R', 'Javascript'], \n",
    "              'AUS_Frequency' : [freq_Python, freq_Java, freq_hadoop, freq_cplus, freq_hive, freq_spark, freq_aws,freq_tableau, freq_sql,freq_pig, freq_c, freq_r, freq_javascript]}\n",
    "AUS_table = DataFrame(freq_table, columns = ['Skill', 'AUS_Frequency'])\n",
    "print(AUS_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual graph for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot LA frequency bar chart\n",
    "\n",
    "LA_graph = LA_table.plot(x = 'Skill', y = 'LA_Frequency', kind = 'bar')\n",
    "LA_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Boston frequency bar chart\n",
    "\n",
    "BOS_graph = BOS_table.plot(x = 'Skill', y = 'BOS_Frequency', kind = 'bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot SF frequency bar chart\n",
    "\n",
    "SF_graph = SF_table.plot(x = 'Skill', y = 'SF_Frequency', kind = 'bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Chicago frequency bar chart\n",
    "CHI_graph = CHI_table.plot(x = 'Skill', y = 'CHI_Frequency', kind = 'bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot NY frequency bar chart\n",
    "NY_graph = NY_table.plot(x = 'Skill', y = 'NY_Frequency', kind = 'bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot DC frequency table\n",
    "DC_graph = DC_table.plot(x = 'Skill', y = 'DC_Frequency', kind = 'bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Austin frequency table\n",
    "AUS_graph = AUS_table.plot(x = 'Skill', y = 'AUS_Frequency', kind = 'bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Comparision Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreqOverview = NY_table.merge(AUS_table, on='Skill').merge(BOS_table, on = 'Skill').merge(LA_table, on='Skill').merge(DC_table, on = 'Skill').merge(CHI_table, on = 'Skill').merge(SF_table, on ='Skill')\n",
    "print(FreqOverview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalGraph = FreqOverview.plot.bar(title = \"Frequency of Skills Required\", legend = True, fontsize = 12)\n",
    "FinalGraph.set_xlabel(\"Skills\", fontsize = 12)\n",
    "FinalGraph.set_ylabel(\"Frequency\", fontsize = 12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
